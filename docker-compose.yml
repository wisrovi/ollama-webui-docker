version: "3.8"

# Define reusable GPU resource constraints
x-resources: &resources_gpu
  shm_size: "64g"
  logging:
    driver: "json-file"
    options:
      max-size: "10m"
      max-file: "3"
  environment:
    - NVIDIA_VISIBLE_DEVICES=0
    - NVIDIA_DRIVER_CAPABILITIES=compute,utility
  deploy:
    resources:
      # limits:
      #   cpus: "6.0"
      #   memory: 16384M
      # memory_swap: 16384M
      reservations:
        cpus: "1.0"
        memory: 2048M
        # memory_swap: 500M
        devices:
          - driver: nvidia
            device_ids: ["0"]
            capabilities: [gpu]


services:
  webui:
    image: ghcr.io/open-webui/open-webui:main
    expose:
     - 8080/tcp
    ports:
     - 8080:8080/tcp
    environment:
     - OLLAMA_BASE_URL=http://ollama:11434
     # uncomment the following if you are running ollama on the docker host and remove the ollama service below
     #- OLLAMA_BASE_URL=http://host.docker.internal:11434
    volumes:
      - open-webui:/app/backend/data
    depends_on:
     - ollama

  ollama:
    <<: *resources_gpu
    image: ollama/ollama
    expose:
     - 11434/tcp
    ports:
     - 11434:11434/tcp
    healthcheck:
      test: ollama --version || exit 1
    command: serve
    volumes:
      - ollama:/root/.ollama

  tunnel:
    image: cloudflare/cloudflared:latest
    restart: unless-stopped
    environment:
      - TUNNEL_URL=http://webui:8080
    command: tunnel --no-autoupdate
    depends_on:
      - webui

volumes:
  ollama:
  open-webui:
